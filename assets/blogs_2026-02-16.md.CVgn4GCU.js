import{_ as t,o as a,c as o,ag as n}from"./chunks/framework.ePeAWSvT.js";const r="/blogs/2026-02-16/01-multi-bots.png",i="/blogs/2026-02-16/02-containerized.png",s="/blogs/2026-02-16/03-memory-engineering.png",y=JSON.parse('{"title":"Introduction to Memoh - The Case for an Always-On, Containerized Home Agent","description":"","frontmatter":{"title":"Introduction to Memoh - The Case for an Always-On, Containerized Home Agent","author":"Team Memoh"},"headers":[],"relativePath":"blogs/2026-02-16.md","filePath":"blogs/2026-02-16.md","lastUpdated":1771238980000}'),l={name:"blogs/2026-02-16.md"};function h(c,e,d,u,m,p){return a(),o("div",null,[...e[0]||(e[0]=[n('<h1 id="introduction-to-memoh-the-case-for-an-always-on-containerized-home-agent" tabindex="-1">Introduction to Memoh - The Case for an Always-On, Containerized Home Agent <a class="header-anchor" href="#introduction-to-memoh-the-case-for-an-always-on-containerized-home-agent" aria-label="Permalink to &quot;Introduction to Memoh - The Case for an Always-On, Containerized Home Agent&quot;">​</a></h1><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>We enter 2026 with a familiar tension: models get smarter every quarter, but the “agent experience” still breaks on context, latency, privacy, and real-world workflows. Over the past year, we kept circling three questions:</p><ul><li>Where does the capability boundary of agents actually sit?</li><li>What’s the real value of long context?</li><li>What hardware form factor makes “always-on, personal AI” feel natural?</li></ul><p>Memoh is our attempt to turn those questions into something buildable—not a manifesto, but a system that can survive contact with reality.</p><h2 id="story-time" tabindex="-1">Story Time <a class="header-anchor" href="#story-time" aria-label="Permalink to &quot;Story Time&quot;">​</a></h2><p>Time travels fast. Somewhere between “I’ll remember this” and “wait, why did we decide that?”, a year disappears.</p><p>That’s the annoying part of building: most progress doesn’t feel like progress while it’s happening. It’s just a stream of small choices, half-finished threads, late-night fixes, and the occasional moment that actually clicks. The kind of moment where you sit back and think: okay—this is real.</p><p>Around the same time, I noticed something else: the internet started to feel smoother—and worse.</p><p>Text got cleaner, longer, more polite, more… empty. You could smell when something was generated: low information density, too many metaphors, too much agreement, not enough stakes.</p><p>I caught myself doing it too.</p><p>So I started forcing a constraint: say it plainly. Keep the density. Don’t inflate. Don’t hide behind style. If something mattered, anchor it to a real moment, a real trade-off, a real cost paid.</p><p>Because the thing LLMs can’t give you is not “intelligence.” It’s weight. The feeling that a human actually stood somewhere in time and wrote from that position.</p><p>That’s when I realized what I wanted wasn’t “an AI that can talk.” I wanted an AI that can live with you—quietly, continuously, accumulating context without turning your life into content sludge.</p><p>Phones were our first instinct—it&#39;s personal, powerful, always there. But mobile OS is closed: without OEM privileges you can build an app, not ambient infrastructure.</p><p>So we looked for the always-on node every home already has: the router (conceptually). Then the economics clash—router-class hardware can’t carry memory, RAG, tools, and multi-user agents. The device evolves: more RAM/storage, a screen, mic/speaker, tiny battery for take out, portable form.</p><p>Eventually it stops being a router. It becomes a new category: a home agent base layer.</p><h2 id="what" tabindex="-1">What <a class="header-anchor" href="#what" aria-label="Permalink to &quot;What&quot;">​</a></h2><p>Memoh is a containerized home/studio AI base layer: cloud-grade model capability paired with local-first memory (knowledge base, RAG/search, conversation history) that stays under your control.</p><h2 id="why" tabindex="-1">Why <a class="header-anchor" href="#why" aria-label="Permalink to &quot;Why&quot;">​</a></h2><p>Long-context models raise the ceiling for agents—but they also make “fully local” expensive and “fully cloud” uncomfortable. People don’t want to re-brief AI every day, and they don’t want their durable context trapped in someone else’s feed. Containerization makes Memoh portable, reproducible, and safe to run as always-on infrastructure—so continuity becomes cheap, private, and dependable.</p><h2 id="how" tabindex="-1">How <a class="header-anchor" href="#how" aria-label="Permalink to &quot;How&quot;">​</a></h2><p>We run Memoh as a containerized stack: isolated services for storage (files/DB/vector index), retrieval, tool/runtime execution, and the control plane. Inference calls cloud APIs when you need frontier capability; durable memory and indexing stay local. The device acts as an always-on node (router-like, not a router) serving multiple users with strict boundaries: sharing is explicit, private context remains private, and everything is deployable/upgradable as versioned containers.</p><h2 id="features" tabindex="-1">Features <a class="header-anchor" href="#features" aria-label="Permalink to &quot;Features&quot;">​</a></h2><ul><li><p><strong>Multi-bot Management</strong>: Create multiple bots; humans and bots, or bots with each other, can chat privately, in groups, or collaborate.</p><p><img src="'+r+'" alt="Multi-bot Management"></p></li><li><p><strong>Containerized</strong>: Each bot runs in its own isolated container. Bots can freely execute commands, edit files, and access the network within their containers—like having their own computer.</p><p><img src="'+i+'" alt="Containerized"></p></li><li><p><strong>Memory Engineering</strong>: Every chat is stored in the database, with the last 24 hours of context loaded by default. Each conversation turn is stored as memory and can be retrieved by bots through semantic search.</p><p><img src="'+s+'" alt="Memory Engineering"></p></li><li><p><strong>Various Platforms</strong>: Supports Telegram, Lark (Feishu), and more.</p></li><li><p><strong>Simple and Easy to Use</strong>: Configure bots and settings for Provider, Model, Memory, Channel, MCP, and Skills through a graphical interface—no coding required to set up your own AI bot.</p></li><li><p><strong>Scheduled Tasks</strong>: Schedule tasks with cron expressions to run commands at specified times.</p></li><li><p>More...</p></li></ul><h2 id="compare-to-openclaw" tabindex="-1">Compare to OpenClaw <a class="header-anchor" href="#compare-to-openclaw" aria-label="Permalink to &quot;Compare to OpenClaw&quot;">​</a></h2><p>We share a core belief: both Memoh and OpenClaw treat the agent as more than a chatbox—we give the LLM a playground: a real environment where it can remember, use tools, and iterate.</p><p>Where Memoh differs:</p><ul><li>Lighter and Faster: built as home/studio infrastructure, can be held in the edge device</li><li><strong>Containerized by default</strong>: each bot gets an isolated container (files/commands/network/jobs)</li><li><strong>Hybrid split</strong>: cloud inference, local-first memory + indexing</li><li><strong>Multi-user first</strong>: explicit sharing and privacy boundaries, support a2a (Agent2Agent)</li><li><strong>Sustainable</strong>: have an experienced team and confidence to push forward and build it</li></ul><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to &quot;Conclusion&quot;">​</a></h2><p>Memoh is built for one thing: always-on continuity—an AI that stays online, and a memory that stays yours.</p><p>We keep frontier inference in the cloud, keep durable context local, and run everything as a containerized, always-on stack. If you want an agent that feels less like an app and more like home infrastructure, that’s the bet Memoh is making.</p><p>Furthermore, we will continue to operate and permanently open-source Memoh, making it a product with long impact.</p>',33)])])}const f=t(l,[["render",h]]);export{y as __pageData,f as default};
