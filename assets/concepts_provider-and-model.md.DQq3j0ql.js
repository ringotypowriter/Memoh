import{_ as t,o,c as d,ag as a}from"./chunks/framework.ePeAWSvT.js";const m=JSON.parse('{"title":"Provider and Model","description":"","frontmatter":{},"headers":[],"relativePath":"concepts/provider-and-model.md","filePath":"concepts/provider-and-model.md","lastUpdated":1771410627000}'),i={name:"concepts/provider-and-model.md"};function r(n,e,l,s,c,p){return o(),d("div",null,[...e[0]||(e[0]=[a('<h1 id="provider-and-model" tabindex="-1">Provider and Model <a class="header-anchor" href="#provider-and-model" aria-label="Permalink to &quot;Provider and Model&quot;">​</a></h1><p>In Memoh, <strong>provider</strong> and <strong>model</strong> are separate but connected concepts:</p><ul><li>A <strong>provider</strong> is the LLM service configuration (API endpoint and key)</li><li>A <strong>model</strong> is the concrete chat or embedding model under that provider, including its <strong>client type</strong> which determines which API protocol to use</li></ul><h2 id="client-types" tabindex="-1">Client Types <a class="header-anchor" href="#client-types" aria-label="Permalink to &quot;Client Types&quot;">​</a></h2><p>Each model has a <code>client_type</code> that determines how Memoh communicates with the LLM service:</p><table tabindex="0"><thead><tr><th>Client Type</th><th>Description</th></tr></thead><tbody><tr><td><code>openai-responses</code></td><td>OpenAI Responses API</td></tr><tr><td><code>openai-completions</code></td><td>OpenAI Chat Completions API (also works with compatible services like Ollama, Mistral, etc.)</td></tr><tr><td><code>anthropic-messages</code></td><td>Anthropic Messages API</td></tr><tr><td><code>google-generative-ai</code></td><td>Google Generative AI API</td></tr></tbody></table><h2 id="typical-setup" tabindex="-1">Typical Setup <a class="header-anchor" href="#typical-setup" aria-label="Permalink to &quot;Typical Setup&quot;">​</a></h2><p>At minimum, a production-ready bot usually needs:</p><ul><li>One <strong>chat</strong> model for dialog generation</li><li>One <strong>embedding</strong> model for memory indexing and retrieval</li></ul><h2 id="model-assignment-to-bot" tabindex="-1">Model Assignment to Bot <a class="header-anchor" href="#model-assignment-to-bot" aria-label="Permalink to &quot;Model Assignment to Bot&quot;">​</a></h2><p>Bots reference model IDs in settings:</p><ul><li><code>chat_model_id</code></li><li><code>memory_model_id</code></li><li><code>embedding_model_id</code></li></ul><p>This enables per-bot customization (for quality, latency, or cost).</p><h2 id="web-ui-path" tabindex="-1">Web UI Path <a class="header-anchor" href="#web-ui-path" aria-label="Permalink to &quot;Web UI Path&quot;">​</a></h2><ul><li><code>Models &gt; Add Provider &gt; Select Provider &gt; Add Model</code></li><li><code>Bots &gt; Select a bot &gt; Settings &gt; Choose chat/memory/embedding models</code></li></ul>',15)])])}const g=t(i,[["render",r]]);export{m as __pageData,g as default};
